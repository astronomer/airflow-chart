######################################
## Logging sidecar configmap        ##
######################################
{{- if and .Values.loggingSidecar.enabled (not .Values.loggingSidecar.customConfig) }}
kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ .Release.Name }}-sidecar-config
  labels:
    tier: airflow
    component: logging-sidecar
    release: {{ .Release.Name }}
    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
    heritage: {{ .Release.Service }}
data:
  vector-config.yaml: |
    log_schema:
      timestamp_key : "@timestamp"
    data_dir: "${SIDECAR_LOGS}"
    sources:
      airflow_log_files:
        type: file
        include:
          - "${SIDECAR_LOGS}/*.log"
        read_from: beginning
    transforms:
      transform_syslog:
        type: add_fields
        inputs:
          - airflow_log_files
        fields:
          component: "${COMPONENT:--}"
          workspace: "${WORKSPACE:--}"
          release: "${RELEASE:--}"

      filter_common_logs:
        type: filter
        inputs:
          - transform_syslog
        condition:
          type: "vrl"
          source: '!includes(["worker"], .component)'

      filter_task_logs:
        type: filter
        inputs:
          - transform_syslog
        condition:
          type: "vrl"
          source: 'includes(["worker"], .component)'

      transform_task_log:
        type: remap
        inputs:
          - filter_task_logs
        source: |-
          # Parse Syslog input. The "!" means that the script should abort on error.
          . = parse_json!(.message)
          .@timestamp = parse_timestamp(.timestamp, "%Y-%m-%dT%H:%M:%S%Z") ?? now()
          .check_log_id = exists(.log_id)
          if .check_log_id != true {
          .log_id = join!([.dag_id, .task_id, .execution_date, .try_number], "_")
          }
          .offset = to_int(now()) * 1000000000 + to_unix_timestamp(now()) * 1000000

      final_task_log:
        type: add_fields
        inputs:
          - transform_task_log
        fields:
          component: "${COMPONENT:--}"
          workspace: "${WORKSPACE:--}"
          release: "${RELEASE:--}"

      transform_remove_fields:
        type: remove_fields
        inputs:
          - final_task_log
          - filter_common_logs
        fields:
          - host
          - file

    sinks:
      out:
        type: elasticsearch
        inputs:
          - transform_remove_fields
        mode: bulk
        compression: none
      {{- if .Values.airflow.elasticsearch.enabled  }}
        endpoint: "http://{{ .Values.airflow.elasticsearch.connection.host }}:{{ .Values.airflow.elasticsearch.connection.port }}"
        auth:
          strategy: "basic"
          user: {{ .Values.airflow.elasticsearch.connection.user }}
          password : {{ .Values.airflow.elasticsearch.connection.pass }}
      {{- end }}
        bulk:
          index: "vector.${RELEASE:--}.%Y.%m.%d"
          action: create
{{- end }}
