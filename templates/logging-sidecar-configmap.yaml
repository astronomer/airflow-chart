######################################
## Logging sidecar configmap        ##
######################################
{{- if and .Values.loggingSidecar.enabled (not .Values.loggingSidecar.customConfig) }}
kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ .Release.Name }}-sidecar-config
  labels:
    tier: airflow
    component: logging-sidecar
    release: {{ .Release.Name }}
    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
    heritage: {{ .Release.Service }}
data:
  vector-config.yaml: |
    log_schema:
      timestamp_key: "@timestamp"
    data_dir: "${SIDECAR_LOGS}"
    sources:
      airflow_log_files:
        type: file
        include:
          - "${SIDECAR_LOGS}/*.log"
          {{- if semverCompare ">=3.0.0" .Values.airflow.airflowVersion }}
          - "/usr/local/airflow/logs/**/*.log"
          {{- end }}
        read_from: beginning
    transforms:
      # Parse Airflow 3 log file paths to extract metadata
      parse_airflow3_path:
        type: remap
        inputs:
          - airflow_log_files
        source: |
        {{- if semverCompare ">=3.0.0" .Values.airflow.airflowVersion }}
          # Airflow 3.x - Parse JSON structured logs
          parsed_log = parse_json(.message) ?? .
          parsed, err = parse_regex(.file, r'/dag_id=(?P<dag_id>[0-9a-zA-Z-_]+)/run_id=(?P<run_id>[^/]+)/task_id=(?P<task_id>[0-9a-zA-Z-_]+)/(?:map_index=(?P<map_index>-?[0-9])/)?attempt=(?P<attempt>[0-9]+)\.log$')

          if err == null {
            # Airflow 3 structured logs
            . = merge!(., parsed_log)
            . = merge(., parsed)

            # Create log_id for consistency
            map_index = get(., ["map_index"]) ?? "-1"
            if is_null(map_index) {
              map_index = "-1"
            }
            .log_id = join!([
                string!(get(., ["dag_id"]) ?? "-"),
                string!(get(., ["task_id"]) ?? "-"),
                string!(get(., ["run_id"]) ?? "-"),
                map_index,
                string!(get(., ["attempt"]) ?? "-")
            ], "_")
          }
        {{- else }}
          # Airflow 2.x - Parse traditional log file paths
          . = parse_json(.message) ?? .
          .@timestamp = parse_timestamp(.timestamp, "%Y-%m-%dT%H:%M:%S%Z") ?? now()
          .check_log_id = exists(.log_id)
          if .check_log_id != true {
            .log_id = join!([to_string!(.dag_id), to_string!(.task_id), to_string!(.execution_date), to_string!(.try_number)], "_")
          }
          .offset = to_int(now()) * 1000000000 + to_unix_timestamp(now()) * 1000000
        {{- end }}

      transform_airflow_logs:
        type: remap
        inputs:
          - parse_airflow3_path
        source: |
          .component = "${COMPONENT:--}"
          .workspace = "${WORKSPACE:--}"
          .release = "${RELEASE:--}"
          .date_nano = parse_timestamp!(.@timestamp, format: "%Y-%m-%dT%H:%M:%S.%f%Z")

      filter_common_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: '!includes(["worker","scheduler"], .component)'

      filter_scheduler_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["scheduler"], .component)'

      filter_worker_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["worker"], .component)'

      filter_gitsyncrelay_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["git-sync-relay"], .component)'

      filter_dagserver_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["dag-server"], .component)'

      filter_airflow_downgrade_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["airflow-downgrade"], .component)'

      filter_apiserver_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["api-server"], .component)'

      transform_task_log:
        type: remap
        inputs:
          - filter_worker_logs
          - filter_scheduler_logs
        source: |-
          # Handle timestamp - use existing or create new
          if !exists(.@timestamp) {
            .@timestamp = now()
          }

          # Calculate offset for ordering
        {{- if semverCompare ">=3.0.0" .Values.airflow.airflowVersion }}
          .offset = to_unix_timestamp(now()) * 1000000
        {{- else }}
          .offset = to_int(now()) * 1000000000 + to_unix_timestamp(now()) * 1000000
        {{- end }}

      final_task_log:
        type: remap
        inputs:
          - transform_task_log
        source: |
          .component = "${COMPONENT:--}"
          .workspace = "${WORKSPACE:--}"
          .release = "${RELEASE:--}"

        {{- if not (semverCompare ">=3.0.0" .Values.airflow.airflowVersion) }}
          if exists(.@timestamp) {
            .date_nano = parse_timestamp!(.@timestamp, format: "%Y-%m-%dT%H:%M:%S.%f%Z")
          }
        {{- end }}

      map_log_level:
        type: remap
        inputs:
          - final_task_log
          - filter_common_logs
          - filter_gitsyncrelay_logs
          - filter_dagserver_logs
          - filter_airflow_downgrade_logs
          - filter_apiserver_logs
        source: |
          # Create level_numeric field for Elasticsearch while keeping level as string for UI
          if exists(.level) && is_string(.level) {
            level_lower = downcase!(.level)
            level_map = {
              "debug": 10,
              "info": 20,
              "warning": 30,
              "warn": 30,
              "error": 40,
              "critical": 50,
              "exception": 50
            }
            .level_numeric = get(level_map, [level_lower]) ?? 20
          }

      handle_error_details:
        type: remap
        inputs:
          - map_log_level
        source: |
          # Extract exception details from error_detail
          if exists(.error_detail) {
            # Encode full error_detail as JSON string with pretty-printing (indentation)
            error_detail_str = encode_json(.error_detail, pretty: true)

            # Append error_detail JSON to event field with newlines for better readability
            if exists(.event) {
              .event = string!(.event) + "\nError Details:\n" + error_detail_str
            }
          }

      transform_remove_fields:
        type: remap
        inputs:
          - handle_error_details
        source: |
          del(.host)
          del(.file)
        {{- if semverCompare ">=3.0.0" .Values.airflow.airflowVersion }}
          del(.execution_date)
        {{- end }}

    sinks:
      out:
        type: elasticsearch
        inputs:
          - transform_remove_fields
        mode: bulk
        compression: none
      {{- if .Values.airflow.elasticsearch.enabled  }}
        endpoint: "http://{{ .Values.airflow.elasticsearch.connection.host }}:{{ .Values.airflow.elasticsearch.connection.port }}"
        auth:
          strategy: "basic"
          user: {{ .Values.airflow.elasticsearch.connection.user }}
          password: {{ .Values.airflow.elasticsearch.connection.pass }}
      {{- end }}
        bulk:
          index: "{{ .Values.loggingSidecar.indexNamePrefix }}.${RELEASE:--}.{{ .Values.loggingSidecar.indexPattern }}"
          action: create
{{- end }}
