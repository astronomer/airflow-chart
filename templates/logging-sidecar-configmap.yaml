######################################
## Logging sidecar configmap        ##
######################################
{{- if and .Values.loggingSidecar.enabled (not .Values.loggingSidecar.customConfig) }}
kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ .Release.Name }}-sidecar-config
  labels:
    tier: airflow
    component: logging-sidecar
    release: {{ .Release.Name }}
    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
    heritage: {{ .Release.Service }}
data:
  vector-config.yaml: |
    log_schema:
      timestamp_key : "@timestamp"
    data_dir: "${SIDECAR_LOGS}"
    sources:
      airflow_log_files:
        type: file
        include:
          - "${SIDECAR_LOGS}/*.log"
          - "${SIDECAR_LOGS}/**/*.log"
        read_from: beginning
    transforms:
      # Parse Airflow 3 log file paths to extract metadata
      parse_airflow3_path:
        type: remap
        inputs:
          - airflow_log_files
        source: |
          # Try to parse Airflow 3 directory structure
          parsed = parse_regex(.file, r'/dag_id=(?P<dagID>[0-9a-zA-Z-_]+)/run_id=(?P<runID>[^/]+)/task_id=(?P<taskID>[0-9a-zA-Z-_]+)/(?:map_index=(?P<mapIndex>-?[0-9]+)/)?attempt=(?P<attempt>[0-9]+)/(?P<tiID>[0-9a-zA-Z-]+)(?:\.log\.trigger\.[0-9]+)?\.log$')
          
          if parsed != null {
            # Airflow 3 structured logs
            .tiID = parsed.tiID
            .attempt = to_int!(parsed.attempt)
            .task_id = parsed.taskID
            .run_id = parsed.runID
            .dag_id = parsed.dagID
            .map_index = if parsed.mapIndex != null { to_int!(parsed.mapIndex) } else { -1 }
            .try_number = .attempt
            .is_airflow3 = true
            # Create log_id for consistency with Airflow 2 format
            .log_id = join!([.dag_id, .task_id, .run_id, to_string!(.try_number)], "_")
          } else {
            # Fallback for non-task logs or Airflow 2 format
            .is_airflow3 = false
          }

      transform_airflow_logs:
        type: remap
        inputs:
          - parse_airflow3_path
        source: |
          .component = "${COMPONENT:--}"
          .workspace = "${WORKSPACE:--}"
          .release = "${RELEASE:--}"
          
          # Parse timestamp if it exists
          if exists(.@timestamp) {
            .date_nano = parse_timestamp!(.@timestamp, format: "%Y-%m-%dT%H:%M:%S.%f%Z")
          }

      filter_common_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: '!includes(["worker","scheduler"], .component)'

      filter_scheduler_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["scheduler"], .component)'

      filter_worker_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["worker"], .component)'

      filter_gitsyncrelay_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["git-sync-relay"], .component)'

      filter_dagserver_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["dag-server"], .component)'

      filter_airflow_downgrade_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["airflow-downgrade"], .component)'

      filter_apiserver_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["api-server"], .component)'

      transform_task_log:
        type: remap
        inputs:
          - filter_worker_logs
          - filter_scheduler_logs
        source: |-
          # Handle timestamp - use existing or create new
          if !exists(.@timestamp) {
            .@timestamp = now()
          }
          
          # Calculate offset for ordering
          .offset = to_int(now()) * 1000000000 + to_unix_timestamp(now()) * 1000000

      final_task_log:
        type: remap
        inputs:
          - transform_task_log
        source: |
          .component = "${COMPONENT:--}"
          .workspace = "${WORKSPACE:--}"
          .release = "${RELEASE:--}"
          
          if exists(.@timestamp) {
            .date_nano = parse_timestamp!(.@timestamp, format: "%Y-%m-%dT%H:%M:%S.%f%Z")
          }

      transform_remove_fields:
        type: remap
        inputs:
          - final_task_log
          - filter_common_logs
          - filter_gitsyncrelay_logs
          - filter_dagserver_logs
          - filter_airflow_downgrade_logs
          - filter_apiserver_logs
        source: |
          del(.host)
          del(.file)
          del(.is_airflow3)

    sinks:
      out:
        type: elasticsearch
        inputs:
          - transform_remove_fields
        mode: bulk
        compression: none
      {{- if .Values.airflow.elasticsearch.enabled  }}
        endpoint: "http://{{ .Values.airflow.elasticsearch.connection.host }}:{{ .Values.airflow.elasticsearch.connection.port }}"
        auth:
          strategy: "basic"
          user: {{ .Values.airflow.elasticsearch.connection.user }}
          password : {{ .Values.airflow.elasticsearch.connection.pass }}
      {{- end }}
        bulk:
          index: "{{ .Values.loggingSidecar.indexNamePrefix }}.${RELEASE:--}.{{ .Values.loggingSidecar.indexPattern }}"
          action: create
{{- end }}