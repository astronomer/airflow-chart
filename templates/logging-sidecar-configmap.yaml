######################################
## Logging sidecar configmap        ##
######################################
{{- if and .Values.loggingSidecar.enabled (not .Values.loggingSidecar.customConfig) }}
kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ .Release.Name }}-sidecar-config
  labels:
    tier: airflow
    component: logging-sidecar
    release: {{ .Release.Name }}
    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
    heritage: {{ .Release.Service }}
data:
  vector-config.yaml: |
    log_schema:
      timestamp_key : "@timestamp"
    data_dir: "${SIDECAR_LOGS}"
    sources:
      airflow_log_files:
        type: file
        include:
          - "${SIDECAR_LOGS}/*.log"
          {{- if semverCompare ">=3.0.0" .Values.airflow.airflowVersion }}
          - "/usr/local/airflow/logs/**/*.log"
          {{- end }}
        read_from: beginning
    transforms:
      # Parse Airflow 3 log file paths to extract metadata
      parse_airflow3_path:
        type: remap
        inputs:
          - airflow_log_files
        source: |
        {{- if semverCompare ">=3.0.0" .Values.airflow.airflowVersion }}
          # Airflow 3.x - Parse JSON structured logs
          parsed_log = parse_json(.message) ?? .
          parsed, err = parse_regex(.file, r'/dag_id=(?P<dagID>[0-9a-zA-Z-_]+)/run_id=(?P<runID>[^/]+)/task_id=(?P<taskID>[0-9a-zA-Z-_]+)/(?:map_index=(?P<mapIndex>-?[0-9]+)/)?attempt=(?P<attempt>[0-9]+)\.log$')

          if err == null {
            # Airflow 3 structured logs
            .attempt = to_int!(parsed.attempt)
            .task_id = parsed.taskID
            .run_id = parsed.runID
            .dag_id = parsed.dagID
            .map_index = if parsed.mapIndex != null { to_int!(parsed.mapIndex) } else { -1 }
            .try_number = .attempt
            .is_airflow3 = true

            # Create log_id for consistency
            .log_id = join!([.dag_id, .task_id, .run_id, to_string(.map_index), to_string(.try_number)], "_")

            # Extract structured log fields
            .event = parsed_log.event
            .timestamp = parsed_log.timestamp
            .level = parsed_log.level
            .logger = parsed_log.logger
            .chan = parsed_log.chan
            # Extract error details if present
            if exists(parsed_log.error_detail) {
              .error_detail = parsed_log.error_detail
            }
          } else {
            # Fallback for non-task logs
            .is_airflow3 = false
          }
        {{- else }}
          # Airflow 2.x - Parse traditional log file paths
          . = parse_json(.message) ?? .
          .@timestamp = parse_timestamp(.timestamp, "%Y-%m-%dT%H:%M:%S%Z") ?? now()
          .check_log_id = exists(.log_id)
          if .check_log_id != true {
          .log_id = join!([to_string!(.dag_id), to_string!(.task_id), to_string!(.execution_date), to_string!(.try_number)], "_")
          }
          .offset = to_int(now()) * 1000000000 + to_unix_timestamp(now()) * 1000000
        {{- end }}

      transform_airflow_logs:
        type: remap
        inputs:
          - parse_airflow3_path
        source: |
          .component = "${COMPONENT:--}"
          .workspace = "${WORKSPACE:--}"
          .release = "${RELEASE:--}"
          .date_nano = parse_timestamp!(.@timestamp, format: "%Y-%m-%dT%H:%M:%S.%f%Z")

      filter_common_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: '!includes(["worker","scheduler"], .component)'

      filter_scheduler_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["scheduler"], .component)'

      filter_worker_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["worker"], .component)'

      filter_gitsyncrelay_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["git-sync-relay"], .component)'

      filter_dagserver_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["dag-server"], .component)'

      filter_airflow_downgrade_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["airflow-downgrade"], .component)'

      filter_apiserver_logs:
        type: filter
        inputs:
          - transform_airflow_logs
        condition:
          type: "vrl"
          source: 'includes(["api-server"], .component)'

      transform_task_log:
        type: remap
        inputs:
          - filter_worker_logs
          - filter_scheduler_logs
        source: |-
          # Handle timestamp - use existing or create new
          if !exists(.@timestamp) {
            .@timestamp = now()
          }

          # Calculate offset for ordering
        {{- if semverCompare ">=3.0.0" .Values.airflow.airflowVersion }}
          .offset = to_unix_timestamp(now()) * 1000000
        {{- else }}
          .offset = to_int(now()) * 1000000000 + to_unix_timestamp(now()) * 1000000
        {{- end }}

      final_task_log:
        type: remap
        inputs:
          - transform_task_log
        source: |
          .component = "${COMPONENT:--}"
          .workspace = "${WORKSPACE:--}"
          .release = "${RELEASE:--}"

        {{- if not (semverCompare ">=3.0.0" .Values.airflow.airflowVersion) }}
          if exists(.@timestamp) {
            .date_nano = parse_timestamp!(.@timestamp, format: "%Y-%m-%dT%H:%M:%S.%f%Z")
          }
        {{- end }}

      map_log_level:
        type: remap
        inputs:
          - final_task_log
          - filter_common_logs
          - filter_gitsyncrelay_logs
          - filter_dagserver_logs
          - filter_airflow_downgrade_logs
          - filter_apiserver_logs
        source: |
          # Create level_numeric field for Elasticsearch while keeping level as string for UI
          if exists(.level) && is_string(.level) {
            level_lower = downcase!(.level)
            level_map = {
              "debug": 10,
              "info": 20,
              "warning": 30,
              "warn": 30,
              "error": 40,
              "critical": 50,
              "exception": 50
            }
            .level_numeric = get(level_map, [level_lower]) ?? 20
          }

      handle_error_details:
        type: remap
        inputs:
          - map_log_level
        source: |
          # Extract exception details from error_detail
          if exists(.error_detail) && is_array(.error_detail) {
            # Store full error_detail as JSON for reference
            .error_detail_json = encode_json(.error_detail)

            # Get first error object
            error_obj, err = get(.error_detail, [0])
            if err == null {
              # Extract exception type and message
              exc_type, _ = get(error_obj, ["exc_type"])
              exc_value, _ = get(error_obj, ["exc_value"])

              if exc_type != null && exc_value != null {
                # Create a new summary field with exception info
                .exception_summary = string!(exc_type) + ": " + string!(exc_value)

                # Replace the event field with summary for UI display
                .event = .exception_summary
              }
            }
          }

      transform_remove_fields:
        type: remap
        inputs:
          - handle_error_details
        source: |
          del(.host)
          del(.file)
          del(.is_airflow3)
        {{- if semverCompare ">=3.0.0" .Values.airflow.airflowVersion }}
          del(.execution_date)
        {{- end }}

    sinks:
      out:
        type: elasticsearch
        inputs:
          - transform_remove_fields
        mode: bulk
        compression: none
      {{- if .Values.airflow.elasticsearch.enabled  }}
        endpoint: "http://{{ .Values.airflow.elasticsearch.connection.host }}:{{ .Values.airflow.elasticsearch.connection.port }}"
        auth:
          strategy: "basic"
          user: {{ .Values.airflow.elasticsearch.connection.user }}
          password : {{ .Values.airflow.elasticsearch.connection.pass }}
      {{- end }}
        bulk:
          index: "{{ .Values.loggingSidecar.indexNamePrefix }}.${RELEASE:--}.{{ .Values.loggingSidecar.indexPattern }}"
          action: create
{{- end }}
